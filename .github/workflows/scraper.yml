name: Pool Scraper

on:
  # Comment out the schedule to disable automatic runs
  schedule:
    # Run every 10 minutes
    - cron: '*/10 * * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Conda
      uses: conda-incubator/setup-miniconda@v2
      with:
        auto-update-conda: true
        python-version: '3.10'
        activate-environment: pool-scraper
        environment-file: environment.yml
        
    - name: Run scraper
      shell: bash -l {0}  # Needed for conda activation
      run: |
        conda activate pool-scraper
        python pool_scraper.py --once
      
    - name: Commit and push if data changed
      run: |
        git config --global user.name 'GitHub Actions'
        git config --global user.email 'actions@github.com'
        git add data/*.csv data/*.png
        # Only commit if there are changes
        git diff --staged --quiet || (git commit -m "Update pool guest data $(date '+%Y-%m-%d %H:%M:%S')" && git push)